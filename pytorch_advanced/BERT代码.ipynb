{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ede779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62be009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a,tokens_b=None):\n",
    "    tokens=['<cls>']+tokens_a+['<sep>']\n",
    "    segments=[0]*(len(tokens_a)+2)\n",
    "    if tokens_b is not None:\n",
    "        tokens+=tokens_b+['<sep>']\n",
    "        segments+=[1]*(len(tokens_b)+1)\n",
    "    return tokens,segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67dd3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self,vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,\n",
    "                num_heads,num_layers,dropout,max_len=100,key_size=768,query_size=768,\n",
    "                value_size=768,**kwargs):\n",
    "        super(BERTEncoder,self).__init__(**kwargs)\n",
    "        self.token_embedding=nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.segment_embedding=nn.Embedding(2,num_hiddens)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f'{i}',d2l.EncoderBlock(key_size,query_size,value_size,\n",
    "                                                        num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout,True))\n",
    "        self.pos_embedding=nn.Parameter(torch.randn(1,max_len,num_hiddens))#可以学习 \n",
    "    def forward(self,tokens,segments,valid_lens):\n",
    "        X=self.token_embedding(tokens)+self.segment_embedding(segments)\n",
    "        X=X+self.pos_embedding.data[:,:X.shape[1],:]\n",
    "        for blk in self.blks:\n",
    "            X=blk(X,valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea4b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size,num_hiddens,ffn_num_hiddens,num_heads=10000,768,1024,4\n",
    "norm_shape,ffn_num_input,num_layers,dropout=[768],768,2,0.2\n",
    "encoder=BERTEncoder(vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,\n",
    "                num_heads,num_layers,dropout)\n",
    "tokens=torch.randint(0,vocab_size,(2,8))\n",
    "segments=torch.tensor([[0,0,0,0,1,1,1,1],[0,0,1,1,1,1,1,1]])\n",
    "encoded_X=encoder(tokens,segments,None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb65ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self,vocab_size,num_hiddens,num_inputs=768,**kwargs):\n",
    "        super(MaskLM,self).__init__(**kwargs)\n",
    "        self.mlp=nn.Sequential(nn.Linear(num_inputs,num_hiddens),\n",
    "                              nn.ReLU(),nn.LayerNorm(num_hiddens),\n",
    "                              nn.Linear(num_hiddens,vocab_size))\n",
    "    def forward(self,X,pred_positions):\n",
    "        num_pred_positions=pred_positions.shape[1]\n",
    "        pred_positions=pred_positions.reshape(-1)\n",
    "        batch_size=X.shape[0]\n",
    "        batch_idx=torch.arange(0,batch_size)\n",
    "        batch_idx=torch.repeat_interleave(batch_idx,num_pred_positions)\n",
    "        masked_X=X[batch_idx,pred_positions]\n",
    "        masked_X=masked_X.reshape((batch_size,num_pred_positions,-1))\n",
    "        mlm_Y_hat=self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1eb7746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm=MaskLM(vocab_size,num_hiddens)\n",
    "mlm_positions=torch.tensor([[1,5,2],[6,1,5]])\n",
    "mlm_Y_hat=mlm(encoded_X,mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7cfba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9.8402, 10.1515,  9.6869,  8.6163,  9.7615,  9.7975],\n",
       "        grad_fn=<NllLossBackward0>),\n",
       " torch.Size([6]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y=torch.tensor([[7,8,9],[6,1,5]])\n",
    "loss=nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l=loss(mlm_Y_hat.reshape(-1,vocab_size),mlm_Y.reshape(-1))\n",
    "mlm_l,mlm_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13439bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NextSentencePrediction\n",
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self,num_inputs,**kwargs):\n",
    "        super(NextSentencePred,self).__init__(**kwargs)\n",
    "        self.output=nn.Linear(num_inputs,2)\n",
    "    def forward(self,X):\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d5b6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    def __init__(self,vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,\n",
    "                num_heads,num_layers,dropout,max_len=1000,key_size=768,mlm_in_features=768,\n",
    "                nsp_in_features=768,**kwargs):\n",
    "        super(BERTModel,self).__init__(**kwargs)\n",
    "        self.encoder=BERTEncoder(vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,\n",
    "                num_heads,num_layers,dropout,max_len=max_len,key_size=key_size)\n",
    "        self.hidden=nn.Sequential(nn.Linear(hid_in_features,num_hiddens),nn.Tanh())\n",
    "        self.mlm=MaskLM(vocab_size,num_hiddens,mlm_in_features)\n",
    "        self.nsp=NextSentencePred(nsp_in_features)\n",
    "    def forward(self,tokens,segments,valid_lens=None,pred_positions=None):\n",
    "        encoded_X=self.encoder(tokens,segments,valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat=self.mlm(encoded_X,pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat=None\n",
    "        nsp_Y_hat=self.nsp(self.hidden(encoded_X[:,0,:]))\n",
    "        return encoded_X,mlm_Y_hat,nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe7987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
