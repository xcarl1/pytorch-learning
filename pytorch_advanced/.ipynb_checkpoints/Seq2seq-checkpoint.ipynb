{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c69abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c9dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现Encoder\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqEncoder,self).__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn=nn.GRU(embed_size,num_hiddens,num_layers,dropout=dropout)\n",
    "    def forward(self,X,*args):\n",
    "        X=self.embedding(X)\n",
    "        X=X.permute(1,0,2)\n",
    "        output,state=self.rnn(X)\n",
    "        return output,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb7a624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder=Seq2SeqEncoder(vocab_size=10,embed_size=8,num_hiddens=16,num_layers=2)\n",
    "X=torch.zeros((4,7),dtype=torch.long)\n",
    "output,state=encoder(X)\n",
    "output.shape,state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538a1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现Decoder\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqDecoder,self).__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn=nn.GRU(embed_size+num_hiddens,num_hiddens,num_layers,dropout=dropout)\n",
    "        self.dense=nn.Linear(num_hiddens,vocab_size)\n",
    "    def init_state(self,enc_outputs,*arg):\n",
    "        return enc_outputs[1]\n",
    "    def forward(self,X,state):\n",
    "        X=self.embedding(X).permute(1,0,2)\n",
    "        context=state[-1].repeat(X.shape[0],1,1)\n",
    "        X_and_context=torch.cat((X,context),2)\n",
    "        output,state=self.rnn(X_and_context,state)\n",
    "        output=self.dense(output).permute(1,0,2)\n",
    "        return output,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c090bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder=Seq2SeqDecoder(vocab_size=10,embed_size=8,num_hiddens=16,num_layers=2)\n",
    "state=decoder.init_state(encoder(X))\n",
    "output,X=decoder(X,state)\n",
    "output.shape,state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e34a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#通过零值化屏蔽掉不相关的项\n",
    "def sequence_mask(X,valid_len,value=0):\n",
    "    maxlen=X.size(1)\n",
    "    mask=torch.arange((maxlen),dtype=torch.float32,device=X.device)[None,:] < valid_len[:,None]\n",
    "    X[~mask]=value\n",
    "    return X\n",
    "X=torch.tensor([[1,2,3],[4,5,6]])\n",
    "sequence_mask(X,torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c544aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self,pred,label,valid_len):\n",
    "        weights=torch.ones_like(label)\n",
    "        weights=sequence_mask(weights,valid_len)\n",
    "        print(weights)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss=super().forward(pred.permute(0,2,1),label)\n",
    "        print(unweighted_loss)\n",
    "        weighted_loss=(unweighted_loss*weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b17bb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 0, 0, 0]])\n",
      "tensor([[2.3026, 2.3026, 2.3026, 2.3026],\n",
      "        [2.3026, 2.3026, 2.3026, 2.3026],\n",
      "        [2.3026, 2.3026, 2.3026, 2.3026]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3,4,10),torch.ones((3,4),dtype=torch.long),torch.tensor([4,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a0616b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练过程\n",
    "# 训练\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        # 如果是线性层\n",
    "        if type(m) == nn.Linear:\n",
    "            # 使用Xavier均匀初始化权重\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        # 如果是GRU层\n",
    "        if type(m) == nn.GRU:\n",
    "            # 对于GRU层的每个参数\n",
    "            for param in m._flat_weights_names:\n",
    "                # 如果是权重参数\n",
    "                if \"weight\" in param:\n",
    "                    # 使用Xavier均匀初始化该权重参数\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    \n",
    "    # 应用xavier_init_weights函数，对网络模型的参数进行初始化\n",
    "    net.apply(xavier_init_weights)\n",
    "    # 将网络模型移动到指定设备上\n",
    "    net.to(device)\n",
    "    # 创建Adam优化器，将网络模型的参数传入优化器\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # 创建MaskedSoftmaxCELoss损失函数对象\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    # 将网络模型设置为训练模式\n",
    "    net.train()\n",
    "    # 创建动画绘制对象，用于绘制损失随训练epoch的变化情况\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',xlim=[10, num_epochs])  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 创建计时器对象，用于计算每个epoch的训练时间\n",
    "        timer = d2l.Timer()\n",
    "        # 创建累加器对象，用于累加损失和标记的数量\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for batch in data_iter:\n",
    "            # 将输入数据移动到指定设备上\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            # 创建起始符号的张量bos，并移动到指定设备上\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                              device=device).reshape(-1,1)\n",
    "            # 构造解码器的输入，将bos和去除最后一列的标签张量Y拼接起来\n",
    "            dec_input = torch.cat([bos, Y[:,:-1]],1)\n",
    "            # 前向传播，得到预测结果Y_hat\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            # 计算损失\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            # 反向传播，计算梯度\n",
    "            l.sum().backward()\n",
    "            # 对梯度进行裁剪，防止梯度爆炸\n",
    "            d2l.grad_clipping(net,1)\n",
    "            # 计算标记的数量\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            # 更新模型参数\n",
    "            optimizer.step()\n",
    "            # 使用torch.no_grad()上下文管理器，关闭梯度计算，避免计算图的构建\n",
    "            with torch.no_grad():\n",
    "                # 累加损失和标记的数量\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        # 每10个epoch打印一次损失\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # 绘制损失随训练epoch的变化情况\n",
    "            animator.add(epoch+1, (metric[0]/metric[1],))   \n",
    "    # 打印最终的损失值和训练速度\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "         f'tokens/sec on {str(device)}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ee8183",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xaf in position 33: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_size,num_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m lr,num_epochs,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m,\u001b[38;5;241m300\u001b[39m,d2l\u001b[38;5;241m.\u001b[39mtry_gpu()\n\u001b[1;32m----> 4\u001b[0m train_iter,src_vocab,tgt_vocab\u001b[38;5;241m=\u001b[39m\u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_nmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m encoder\u001b[38;5;241m=\u001b[39mSeq2SeqEncoder(\u001b[38;5;28mlen\u001b[39m(src_vocab),embed_size,num_hiddens,num_layers,dropout\u001b[38;5;241m=\u001b[39mdropout)\n\u001b[0;32m      6\u001b[0m decoder\u001b[38;5;241m=\u001b[39mSeq2SeqDecoder(\u001b[38;5;28mlen\u001b[39m(tgt_vocab),embed_size,num_hiddens,num_layers,dropout\u001b[38;5;241m=\u001b[39mdropout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\d2l\\torch.py:929\u001b[0m, in \u001b[0;36mload_data_nmt\u001b[1;34m(batch_size, num_steps, num_examples)\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data_nmt\u001b[39m(batch_size, num_steps, num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m):\n\u001b[0;32m    926\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the iterator and the vocabularies of the translation dataset.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m    Defined in :numref:`subsec_mt_data_loading`\"\"\"\u001b[39;00m\n\u001b[1;32m--> 929\u001b[0m     text \u001b[38;5;241m=\u001b[39m preprocess_nmt(\u001b[43mread_data_nmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    930\u001b[0m     source, target \u001b[38;5;241m=\u001b[39m tokenize_nmt(text, num_examples)\n\u001b[0;32m    931\u001b[0m     src_vocab \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mVocab(source, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    932\u001b[0m                           reserved_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<bos>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\d2l\\torch.py:861\u001b[0m, in \u001b[0;36mread_data_nmt\u001b[1;34m()\u001b[0m\n\u001b[0;32m    859\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mdownload_extract(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfra-eng\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfra.txt\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xaf in position 33: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "embed_size,num_hiddens,num_layers,dropout=32,32,2,0.1\n",
    "batch_size,num_steps=64,10\n",
    "lr,num_epochs,device=0.005,300,d2l.try_gpu()\n",
    "train_iter,src_vocab,tgt_vocab=d2l.load_data_nmt(batch_size,num_steps)\n",
    "encoder=Seq2SeqEncoder(len(src_vocab),embed_size,num_hiddens,num_layers,dropout=dropout)\n",
    "decoder=Seq2SeqDecoder(len(tgt_vocab),embed_size,num_hiddens,num_layers,dropout=dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c216e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
