{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e9ebb2-d258-4717-adec-31fac71386af",
   "metadata": {},
   "source": [
    "# 简单的线性回归实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44f777b3-8539-4e2d-a720-84327d325d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_data=torch.tensor([[1.0],[2.0],[3.0]])\n",
    "y_data=torch.tensor([[2.0],[4.0],[6.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0aa07-6f41-4ba3-8f89-8797580ac9b0",
   "metadata": {},
   "source": [
    "# 继承类\n",
    "把我们的模型定义成一个类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cad65efd-bff4-49f3-b08a-84b91bb30fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__init__()和forward()必须有\n",
    "class LinearModel(torch.nn.Module): #继承自Module\n",
    "    def __init__(self): #构造函数\n",
    "        super(LinearModel,self).__init__() #调用父类的构造函数\n",
    "        self.linear=torch.nn.Linear(1,1) #这是个类，类后面加括号实际上是构造一个对象。neural network\n",
    "        #torch.nn.Linear(1,1)中的1应该指的是每个样本中的x和y的维数，比如本题中每个样本x和y的维数都是1\n",
    "        #初始的w和b应该是随机给的\n",
    "    def forward(self,x):\n",
    "        y_pred=self.linear(x) #调用了对象中的__call__() \n",
    "        return y_pred  \n",
    "model=LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863ebeb-2be2-43d8-b0bd-3bb5b22bda8c",
   "metadata": {},
   "source": [
    "# 构造损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44460730-691b-42f5-bc52-f6502c1b8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.MSELoss(size_average=False) #MSELoss也是继承自Module的,False不求1/n\n",
    "#会自动知道权重，对哪个进行求导\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01) #优化器不会构造计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390bbf1-617a-44fe-be5c-951b5c840a2b",
   "metadata": {},
   "source": [
    "# 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae13b085-6e82-41a6-a8fd-345a438c470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(53.0020, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(23.5960, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(10.5052, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(4.6776, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(2.0832, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(0.9283, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(0.4142, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(0.1853, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "10 tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "11 tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "12 tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "13 tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "14 tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "15 tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "16 tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "17 tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "18 tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "19 tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "20 tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "21 tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "22 tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "23 tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "24 tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "25 tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "26 tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "27 tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "28 tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "29 tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "30 tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "31 tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "32 tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "33 tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "34 tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "35 tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "36 tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "37 tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "38 tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "39 tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "40 tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "41 tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "42 tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "43 tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "44 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "45 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "46 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "47 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "48 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "49 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "50 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "51 tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "52 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "53 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "54 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "55 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "56 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "57 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "58 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "59 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "60 tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "61 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "62 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "63 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "64 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "65 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "66 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "67 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "68 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "69 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "70 tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "71 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "72 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "73 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "74 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "75 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "76 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "77 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "78 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "79 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "80 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "81 tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "82 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "83 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "84 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "85 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "86 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "87 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "88 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "89 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "90 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "91 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "92 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "93 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "94 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "95 tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "96 tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "97 tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "98 tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "99 tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "w= 1.9862312078475952\n",
      "b= 0.03129969537258148\n",
      "y_pred= 7.976224422454834\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    y_pred=model(x_data) #传入3*1的data，输出3*1的y_pred\n",
    "    loss=criterion(y_pred,y_data) #loss是三个数据loss的求和\n",
    "    print(epoch,loss)\n",
    "\n",
    "    optimizer.zero_grad() #清空梯度\n",
    "    loss.backward() #反向传播求loss的梯度\n",
    "    optimizer.step() #相当于更新参数，w-=lr*梯度\n",
    "print(\"w=\",model.linear.weight.item())\n",
    "print(\"b=\",model.linear.bias.item())\n",
    "\n",
    "#Text Model\n",
    "x_test=torch.tensor([[4.0]])\n",
    "y_test=model(x_test)\n",
    "print(\"y_pred=\",y_test.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889fed2-7d0d-441b-8b2e-05af213828f5",
   "metadata": {},
   "source": [
    "## 过程\n",
    "# Prepare dataset(准备数据)\n",
    "# Design model using Class:inherit from nn.Module\n",
    "# Construct loss and optimizer:using pytorch\n",
    "# Traing cycle:forward,backward,update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
